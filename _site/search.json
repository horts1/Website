[
  {
    "objectID": "python/Project_Risk.html",
    "href": "python/Project_Risk.html",
    "title": "Project Risk Example",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\n\n# Create dummy data\ndata = {\n    'ProjectID': range(1, 101),\n    'Duration': np.random.randint(10, 50, size=100),\n    'Budget': np.random.randint(50, 200, size=100),\n    'TeamSize': np.random.randint(5, 20, size=100),\n    'Complexity': np.random.randint(1, 10, size=100),\n    'PriorIssues': np.random.randint(0, 5, size=100),\n    'RiskEvent': np.random.randint(0, 2, size=100)  # 0 or 1\n}\n\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n\n\n\n\nProjectID\nDuration\nBudget\nTeamSize\nComplexity\nPriorIssues\nRiskEvent\n\n\n\n\n0\n1\n45\n76\n9\n5\n3\n0\n\n\n1\n2\n22\n161\n13\n4\n4\n1\n\n\n2\n3\n19\n169\n17\n6\n0\n1\n\n\n3\n4\n29\n121\n8\n8\n1\n0\n\n\n4\n5\n17\n60\n15\n2\n0\n1"
  },
  {
    "objectID": "python/Project_Risk.html#predicting-if-there-is-going-to-be-a-risk-event-during-a-project",
    "href": "python/Project_Risk.html#predicting-if-there-is-going-to-be-a-risk-event-during-a-project",
    "title": "Project Risk Example",
    "section": "Predicting if there is going to be a Risk Event during a project:",
    "text": "Predicting if there is going to be a Risk Event during a project:\n\nLogistic Regression\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Split the data into features (X) and target (y)\nX = df[['Duration', 'Budget', 'TeamSize', 'Complexity', 'PriorIssues']]\ny = df['RiskEvent']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize and train the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\n# conf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"Logistic Regression Accuracy is:\", format(accuracy,\".0%\"))\n\n# print(class_report)\n\n\nLogistic Regression Accuracy is: 50%\n\n\n\n\nRandom Forest\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Initialize and train the Random Forest model with hyperparameter tuning\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=0)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters from grid search\nbest_params = grid_search.best_params_\n\n# Train the best model\nbest_rf = grid_search.best_estimator_\nbest_rf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = best_rf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\n# conf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"Random Forest Accuracy is:\", format(accuracy,\".0%\"))\n\n# print(class_report)\n\n# best_params\n\n\nRandom Forest Accuracy is: 50%\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Extract feature importances from the best model\nfeature_importances = best_rf.feature_importances_\n\n# Create a DataFrame for better visualization\nfeatures_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\n\n# Sort the DataFrame by importance\nfeatures_df = features_df.sort_values(by='Importance', ascending=False)\n\n# Display the feature importances\nprint(\"Feature Importances:\")\nprint(features_df)\n\n# Plotting the feature importances\nplt.figure(figsize=(9, 5))\nsns.barplot(x='Importance', y='Feature', data=features_df, palette='viridis')\nplt.title('Feature Importances in Random Forest Model')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.show()\n\n\nFeature Importances:\n       Feature  Importance\n1       Budget    0.389014\n0     Duration    0.203648\n2     TeamSize    0.159826\n3   Complexity    0.141312\n4  PriorIssues    0.106200"
  },
  {
    "objectID": "python/Project_Cost.html",
    "href": "python/Project_Cost.html",
    "title": "Project Cost Estimation Example",
    "section": "",
    "text": "Example of Dummy Data\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create dummy data\nnp.random.seed(42)  # For reproducibility\n\ndata = {\n    'ProjectID': range(1, 101),\n    'Duration': np.random.randint(10, 50, size=100),\n    'TeamSize': np.random.randint(5, 20, size=100),\n    'Complexity': np.random.randint(1, 10, size=100),\n    'InitialBudget': np.random.randint(50, 200, size=100),\n    'PriorIssues': np.random.randint(0, 5, size=100),\n    'ProjectType': np.random.choice(['Residential', 'Commercial', 'Industrial'], size=100),\n    'TeamExperience': np.random.randint(1, 15, size=100),\n    'ExternalFactors': np.random.uniform(0.5, 1.5, size=100),\n    'ResourceAvailability': np.random.randint(1, 10, size=100),\n    'ProjectPhase': np.random.choice(['Design', 'Foundation', 'Structure', 'Finishing'], size=100),\n    'ActualCost': np.random.randint(60, 250, size=100)\n}\n\ndf = pd.DataFrame(data)\n\n# first few rows of the dataframe\ndf.head()\n\n\n\n\n\n\n\n\n\nProjectID\nDuration\nTeamSize\nComplexity\nInitialBudget\nPriorIssues\nProjectType\nTeamExperience\nExternalFactors\nResourceAvailability\nProjectPhase\nActualCost\n\n\n\n\n0\n1\n48\n11\n8\n117\n4\nCommercial\n5\n0.577735\n2\nFoundation\n121\n\n\n1\n2\n38\n11\n9\n82\n3\nCommercial\n6\n1.474395\n3\nDesign\n191\n\n\n2\n3\n24\n18\n4\n191\n4\nCommercial\n3\n1.486211\n3\nFoundation\n148\n\n\n3\n4\n17\n12\n1\n70\n3\nCommercial\n8\n1.198162\n5\nStructure\n101\n\n\n4\n5\n30\n9\n1\n97\n2\nCommercial\n13\n1.036096\n5\nStructure\n148\n\n\n\n\n\n\n\n\n\nData Prep\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Convert categorical features to numerical values\nlabel_encoders = {}\ncategorical_features = ['ProjectType', 'ProjectPhase']\nfor feature in categorical_features:\n    le = LabelEncoder()\n    df[feature] = le.fit_transform(df[feature])\n    label_encoders[feature] = le\n\n# Split the data into features (X) and target (y)\nX = df.drop(columns=['ProjectID', 'ActualCost'])\ny = df['ActualCost']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\nModel Training\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Initialize the RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Output the results\nprint(\"Mean Squared Error:\", mse)\nprint(\"R^2 Score:\", r2)\n\n\nMean Squared Error: 3473.57541\nR^2 Score: -0.1123097958274566\n\n\n\n\nHyperparameter Tuning\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Initialize GridSearchCV with 3-fold cross-validation\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=0)\n\n# Fit GridSearchCV to the data\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and model performance\nbest_params = grid_search.best_params_\nbest_rf = grid_search.best_estimator_\n\n# print(\"Best Parameters from GridSearchCV:\")\n# print(best_params)\n\n# Predict on the test set with the best model\ny_pred_best = best_rf.predict(X_test)\n\n# Evaluate the best model\nmse_best = mean_squared_error(y_test, y_pred_best)\nr2_best = r2_score(y_test, y_pred_best)\n\n# Output the results\nprint(\"Mean Squared Error (Best Model):\", mse_best)\nprint(\"R^2 Score (Best Model):\", r2_best)\n\n\nMean Squared Error (Best Model): 3345.293177772035\nR^2 Score (Best Model): -0.07123120483818601\n\n\n\n\nFeature Importance\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Extract feature importances from the best model\nfeature_importances = best_rf.feature_importances_\n\n# Create a DataFrame for better visualization\nfeatures_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\n\n# Sort the DataFrame by importance\nfeatures_df = features_df.sort_values(by='Importance', ascending=False)\n\n# Display the feature importances\nprint(\"Feature Importances:\")\nprint(features_df)\n\n# Plotting the feature importances\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=features_df, palette='viridis')\nplt.title('Feature Importances in Random Forest Model')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.show()\n\n\nFeature Importances:\n                Feature  Importance\n8  ResourceAvailability    0.184655\n3         InitialBudget    0.172776\n1              TeamSize    0.156746\n0              Duration    0.111923\n7       ExternalFactors    0.105970\n6        TeamExperience    0.092655\n4           PriorIssues    0.066102\n2            Complexity    0.062543\n5           ProjectType    0.029834\n9          ProjectPhase    0.016797"
  },
  {
    "objectID": "python/Credit.html",
    "href": "python/Credit.html",
    "title": "Credit",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport os\nos.getcwd()\ndata_info = pd.read_csv(\n    'c:\\\\Users\\\\HoraceTsai\\\\Documents\\\\Jupyter\\\\TensorFlow_FILES\\DATA\\\\lending_club_info.csv',\n    index_col='LoanStatNew'\n    )\n\n\n\n\nCode\nprint(data_info.loc['revol_util']['Description'])\n\n\nRevolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n\n\n\n\nCode\ndef feat_info(col_name):\n    print(data_info.loc[col_name]['Description'])\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# might be needed depending on your version of Jupyter\n%matplotlib inline\n\n\n\n\nCode\ndf = pd.read_csv('c:\\\\Users\\\\HoraceTsai\\\\Documents\\\\Jupyter\\\\TensorFlow_FILES\\DATA\\\\lending_club_loan_two.csv')\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 396030 entries, 0 to 396029\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             396030 non-null  float64\n 1   term                  396030 non-null  object \n 2   int_rate              396030 non-null  float64\n 3   installment           396030 non-null  float64\n 4   grade                 396030 non-null  object \n 5   sub_grade             396030 non-null  object \n 6   emp_title             373103 non-null  object \n 7   emp_length            377729 non-null  object \n 8   home_ownership        396030 non-null  object \n 9   annual_inc            396030 non-null  float64\n 10  verification_status   396030 non-null  object \n 11  issue_d               396030 non-null  object \n 12  loan_status           396030 non-null  object \n 13  purpose               396030 non-null  object \n 14  title                 394274 non-null  object \n 15  dti                   396030 non-null  float64\n 16  earliest_cr_line      396030 non-null  object \n 17  open_acc              396030 non-null  float64\n 18  pub_rec               396030 non-null  float64\n 19  revol_bal             396030 non-null  float64\n 20  revol_util            395754 non-null  float64\n 21  total_acc             396030 non-null  float64\n 22  initial_list_status   396030 non-null  object \n 23  application_type      396030 non-null  object \n 24  mort_acc              358235 non-null  float64\n 25  pub_rec_bankruptcies  395495 non-null  float64\n 26  address               396030 non-null  object \ndtypes: float64(12), object(15)\nmemory usage: 81.6+ MB\n\n\n\nExploratory Data Analysis (EDA)\n\n\nCode\nsns.countplot(\n    x= \"loan_status\",\n    data= df)\n#inbalanced outcomes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my professional portfolio! I am Horace Tsai, a recent graduate from California State University, Fullerton (CSUF) with a Master of Science in Statistics. In 2019, received my Bachelor of Arts in Economics with minors in Accounting and Statistics at the University of California, Irvine (UCI).\nCurrently, I am working as an Associate Program Manager, where I leverage my analytics skills and passion for data to drive projects and deliver actionable insights.\nDuring my time at CSUF, I developed a strong foundation in statistical theory, data analysis, and machine learning. My academic journey was filled with hands-on projects that honed my ability to turn complex data into clear, meaningful narratives. Whether it’s through advanced statistical modeling, data visualization, or predictive analytics, I strive to uncover patterns and insights that can drive better decision-making.\nMy professional aspiration is to transition into a full-fledged data scientist role. I am eager to apply my skills in a more focused data-centric capacity, tackling challenging problems and contributing to innovative solutions. On this website, you’ll find a showcase of my past projects, each demonstrating my proficiency in various statistical and analytical techniques, as well as my commitment to continuous learning and growth.\nFrom exploratory data analysis to building machine learning models, my projects highlight my technical expertise and my ability to translate data into strategic business value. I invite you to explore my work, and I am always open to connecting with like-minded professionals and potential collaborators.\nThank you for visiting, and I look forward to sharing my journey with you!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "About",
    "section": "",
    "text": "Welcome to my professional portfolio! I am Horace Tsai, a recent graduate from California State University, Fullerton (CSUF) with a Master of Science in Statistics. In 2019, received my Bachelor of Arts in Economics with minors in Accounting and Statistics at the University of California, Irvine (UCI).\nCurrently, I am working as an Associate Program Manager, where I leverage my analytics skills and passion for data to drive projects and deliver actionable insights.\nDuring my time at CSUF, I developed a strong foundation in statistical theory, data analysis, and machine learning. My academic journey was filled with hands-on projects that honed my ability to turn complex data into clear, meaningful narratives. Whether it’s through advanced statistical modeling, data visualization, or predictive analytics, I strive to uncover patterns and insights that can drive better decision-making.\nMy professional aspiration is to transition into a full-fledged data scientist role. I am eager to apply my skills in a more focused data-centric capacity, tackling challenging problems and contributing to innovative solutions. On this website, you’ll find a showcase of my past projects, each demonstrating my proficiency in various statistical and analytical techniques, as well as my commitment to continuous learning and growth.\nFrom exploratory data analysis to building machine learning models, my projects highlight my technical expertise and my ability to translate data into strategic business value. I invite you to explore my work, and I am always open to connecting with like-minded professionals and potential collaborators.\nThank you for visiting, and I look forward to sharing my journey with you!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my professional portfolio! I am Horace Tsai, a recent graduate from California State University, Fullerton (CSUF) with a Master of Science in Statistics. In 2019, received my Bachelor of Arts in Economics with a double minor in Accounting and Statistics at the University of California, Irvine (UCI).\nCurrently, I am working as an Associate Program Manager, where I leverage my analytics skills and passion for data to impactfully drive projects and deliver actionable insights.\nDuring my time at CSUF, I developed a strong foundation in statistical theory, data analysis, and machine learning. My academic journey was filled with hands-on projects that honed my ability to turn complex data into clear, meaningful narratives. Whether it’s through advanced statistical modeling, data visualization, or predictive analytics, I strive to uncover patterns and insights that can drive better decision-making.\nMy professional aspiration is to transition into a full-fledged data scientist role. I am eager to apply my skills in a more focused data-centric capacity, tackling challenging problems and contributing to innovative solutions. On this website, you’ll find a showcase of my past projects, each demonstrating my proficiency in various statistical and analytical techniques, as well as my commitment to continuous learning and growth.\nFrom exploratory data analysis to building machine learning models, my projects highlight my technical expertise and my ability to translate data into strategic business value. I invite you to explore my work, and I am always open to connecting with like-minded professionals and potential collaborators.\nThank you for visiting, and I look forward to sharing my journey with you!"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Welcome to my professional portfolio! I am Horace Tsai, a recent graduate from California State University, Fullerton (CSUF) with a Master of Science in Statistics. In 2019, received my Bachelor of Arts in Economics with a double minor in Accounting and Statistics at the University of California, Irvine (UCI).\nCurrently, I am working as an Associate Program Manager, where I leverage my analytics skills and passion for data to impactfully drive projects and deliver actionable insights.\nDuring my time at CSUF, I developed a strong foundation in statistical theory, data analysis, and machine learning. My academic journey was filled with hands-on projects that honed my ability to turn complex data into clear, meaningful narratives. Whether it’s through advanced statistical modeling, data visualization, or predictive analytics, I strive to uncover patterns and insights that can drive better decision-making.\nMy professional aspiration is to transition into a full-fledged data scientist role. I am eager to apply my skills in a more focused data-centric capacity, tackling challenging problems and contributing to innovative solutions. On this website, you’ll find a showcase of my past projects, each demonstrating my proficiency in various statistical and analytical techniques, as well as my commitment to continuous learning and growth.\nFrom exploratory data analysis to building machine learning models, my projects highlight my technical expertise and my ability to translate data into strategic business value. I invite you to explore my work, and I am always open to connecting with like-minded professionals and potential collaborators.\nThank you for visiting, and I look forward to sharing my journey with you!"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here are the links to some of my projects:"
  },
  {
    "objectID": "Projects.html#r-projects",
    "href": "Projects.html#r-projects",
    "title": "Projects",
    "section": "R Projects:",
    "text": "R Projects:\n\nFrom Data to Diagnosis: Predicting Heart Disease Severity through Machine Learning"
  },
  {
    "objectID": "Projects.html#python-projects",
    "href": "Projects.html#python-projects",
    "title": "Projects",
    "section": "Python Projects:",
    "text": "Python Projects:\n\nShady Business: Testing the Power of Dark mode\ninsert link"
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Python Projects",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\nShady Business: Testing the Power of Dark mode\nLoan Status\nTime Series Example\nProject Risk Example\nProject Resource Allocation Example\nProject Cost Example"
  },
  {
    "objectID": "python/Project_Resource.html",
    "href": "python/Project_Resource.html",
    "title": "Project Resource Allocation Example",
    "section": "",
    "text": "Example of Dummy Data\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create dummy data\nnp.random.seed(42)  # For reproducibility\n\ndata = {\n    'ProjectID': range(1, 101),\n    'Duration': np.random.randint(10, 50, size=100),\n    'TeamSize': np.random.randint(5, 20, size=100),\n    'Complexity': np.random.randint(1, 10, size=100),\n    'InitialBudget': np.random.randint(50, 200, size=100),\n    'PriorIssues': np.random.randint(0, 5, size=100),\n    'ProjectType': np.random.choice(['Residential', 'Commercial', 'Industrial'], size=100),\n    'TeamExperience': np.random.randint(1, 15, size=100),\n    'ExternalFactors': np.random.uniform(0.5, 1.5, size=100),\n    'ResourceAvailability': np.random.randint(1, 10, size=100),\n    'ProjectPhase': np.random.choice(['Design', 'Foundation', 'Structure', 'Finishing'], size=100),\n    'ActualCost': np.random.randint(60, 250, size=100)\n}\n\ndf = pd.DataFrame(data)\n\n# first few rows of the dataframe\ndf.head()\n\n\n\n\n\n\n\n\n\nProjectID\nDuration\nTeamSize\nComplexity\nInitialBudget\nPriorIssues\nProjectType\nTeamExperience\nExternalFactors\nResourceAvailability\nProjectPhase\nActualCost\n\n\n\n\n0\n1\n48\n11\n8\n117\n4\nCommercial\n5\n0.577735\n2\nFoundation\n121\n\n\n1\n2\n38\n11\n9\n82\n3\nCommercial\n6\n1.474395\n3\nDesign\n191\n\n\n2\n3\n24\n18\n4\n191\n4\nCommercial\n3\n1.486211\n3\nFoundation\n148\n\n\n3\n4\n17\n12\n1\n70\n3\nCommercial\n8\n1.198162\n5\nStructure\n101\n\n\n4\n5\n30\n9\n1\n97\n2\nCommercial\n13\n1.036096\n5\nStructure\n148\n\n\n\n\n\n\n\n\n\nData Preps\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Convert categorical features to numerical values\nlabel_encoders = {}\ncategorical_features = ['ProjectType', 'ProjectPhase']\nfor feature in categorical_features:\n    le = LabelEncoder()\n    df[feature] = le.fit_transform(df[feature])\n    label_encoders[feature] = le\n\n# Split the data into features (X) and target (y)\nX = df.drop(columns=['ProjectID', 'ActualCost'])\ny = df['ActualCost']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n\n\nModel Training and Evaluation\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Initialize the RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Output the results\nprint(\"Mean Squared Error:\", mse)\nprint(\"R^2 Score:\", r2)\n\n\nMean Squared Error: 3473.57541\nR^2 Score: -0.1123097958274566\n\n\n\n\nHyperparameter Tuning\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Initialize GridSearchCV with 3-fold cross-validation\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n\n# Fit GridSearchCV to the data\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and model performance\nbest_params = grid_search.best_params_\nbest_rf = grid_search.best_estimator_\n\n#print(\"Best Parameters from GridSearchCV:\")\n#print(best_params)\n\n# Predict on the test set with the best model\ny_pred_best = best_rf.predict(X_test)\n\n# Evaluate the best model\nmse_best = mean_squared_error(y_test, y_pred_best)\nr2_best = r2_score(y_test, y_pred_best)\n\n# Output the results\nprint(\"Mean Squared Error (Best Model):\", mse_best)\nprint(\"R^2 Score (Best Model):\", r2_best)\n\n\nFitting 3 folds for each of 108 candidates, totalling 324 fits\nMean Squared Error (Best Model): 3345.293177772035\nR^2 Score (Best Model): -0.07123120483818601\n\n\n\n\nFeature Importance\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Extract feature importances from the best model\nfeature_importances = best_rf.feature_importances_\n\n# Create a DataFrame for better visualization\nfeatures_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\n\n# Sort the DataFrame by importance\nfeatures_df = features_df.sort_values(by='Importance', ascending=False)\n\n# Display the feature importances\nprint(\"Feature Importances:\")\nprint(features_df)\n\n# Plotting the feature importances\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=features_df, palette='viridis')\nplt.title('Feature Importances in Random Forest Model')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.show()\n\n\nFeature Importances:\n                Feature  Importance\n8  ResourceAvailability    0.184655\n3         InitialBudget    0.172776\n1              TeamSize    0.156746\n0              Duration    0.111923\n7       ExternalFactors    0.105970\n6        TeamExperience    0.092655\n4           PriorIssues    0.066102\n2            Complexity    0.062543\n5           ProjectType    0.029834\n9          ProjectPhase    0.016797"
  },
  {
    "objectID": "python/Time_Series.html",
    "href": "python/Time_Series.html",
    "title": "Time Series Example",
    "section": "",
    "text": "Example of Dummy Data\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nimport warnings\n\n# Dummy dataset\ndata = {\n    'Month': pd.date_range(start='2020-01-01', end='2023-12-01', freq='MS'),\n    'SuccessfulProjects': [10, 12, 15, 11, 13, 17, 14, 18, 16, 19, 17, 20] * 4,\n    'AvgHoursWorked': [160, 170, 150, 180, 140, 190, 165, 175, 155, 170, 160, 180] * 4,\n    'TeamSize': [5, 6, 7, 5, 8, 10, 9, 11, 6, 7, 8, 9] * 4,\n    'ProjectComplexity': [3, 4, 5, 6, 4, 7, 5, 6, 4, 5, 6, 7] * 4\n}\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n\n\n\n\nMonth\nSuccessfulProjects\nAvgHoursWorked\nTeamSize\nProjectComplexity\n\n\n\n\n0\n2020-01-01\n10\n160\n5\n3\n\n\n1\n2020-02-01\n12\n170\n6\n4\n\n\n2\n2020-03-01\n15\n150\n7\n5\n\n\n3\n2020-04-01\n11\n180\n5\n6\n\n\n4\n2020-05-01\n13\n140\n8\n4\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\nCode\ndf.set_index('Month', inplace=True)\n\n# Plotting the data\ndf.plot(figsize=(12, 8), subplots=True)\nplt.show()\n\n# Decompose the time series (for successful projects only, for simplicity)\ndecomposition = seasonal_decompose(df['SuccessfulProjects'], model='additive')\nfig = decomposition.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Building\n\n\nCode\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", message=\"no frequency information was provided\")\n\n# Splitting the data into training and testing sets\ntrain = df[:'2022']\ntest = df['2023':]\n\n# Building the ARIMAX model with adjustments to improve convergence\nmodel = SARIMAX(\n    train['SuccessfulProjects'], \n    exog=train[['AvgHoursWorked', 'TeamSize', 'ProjectComplexity']], \n    order=(1, 1, 1),\n    enforce_stationarity=False,\n    enforce_invertibility=False\n)\nmodel_fit = model.fit(disp=False, maxiter=500, method='nm')\n\n# Summary of the model\nprint(model_fit.summary())\n\n# Forecasting\nforecast = model_fit.get_forecast(steps=len(test), exog=test[['AvgHoursWorked', 'TeamSize', 'ProjectComplexity']])\nforecast_df = test.copy()\nforecast_df['Forecast'] = forecast.predicted_mean\n\n# Plotting the actual vs forecasted values\nplt.figure(figsize=(12, 8))\nplt.plot(train['SuccessfulProjects'], label='Training Data')\nplt.plot(test['SuccessfulProjects'], label='Actual Data')\nplt.plot(forecast_df['Forecast'], label='Forecasted Data', linestyle='--')\nplt.title('Project Success Forecast with Additional Variables')\nplt.xlabel('Date')\nplt.ylabel('Number of Successful Projects')\nplt.legend()\nplt.show()\n\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:     SuccessfulProjects   No. Observations:                   36\nModel:               SARIMAX(1, 1, 1)   Log Likelihood                 -69.563\nDate:                Wed, 10 Jul 2024   AIC                            151.127\nTime:                        15:09:26   BIC                            160.106\nSample:                    01-01-2020   HQIC                           154.148\n                         - 12-01-2022                                         \nCovariance Type:                  opg                                         \n=====================================================================================\n                        coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nAvgHoursWorked       -0.0284      0.041     -0.690      0.490      -0.109       0.052\nTeamSize              0.7813      0.249      3.139      0.002       0.293       1.269\nProjectComplexity     1.1319      0.612      1.848      0.065      -0.068       2.332\nar.L1                 0.5053      0.257      1.967      0.049       0.002       1.009\nma.L1                -1.0000   1458.798     -0.001      0.999   -2860.192    2858.191\nsigma2                3.6376   5306.701      0.001      0.999   -1.04e+04    1.04e+04\n===================================================================================\nLjung-Box (L1) (Q):                   0.03   Jarque-Bera (JB):                 1.79\nProb(Q):                              0.86   Prob(JB):                         0.41\nHeteroskedasticity (H):               0.91   Skew:                             0.31\nProb(H) (two-sided):                  0.88   Kurtosis:                         2.05\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Evaluation metrics\nmae = mean_absolute_error(test['SuccessfulProjects'], forecast.predicted_mean)\nmse = mean_squared_error(test['SuccessfulProjects'], forecast.predicted_mean)\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\n\n\nMean Absolute Error: 1.8805247500041948\nMean Squared Error: 4.89588917787364"
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "R Studio Projects",
    "section": "",
    "text": "From Data to Diagnosis: Predicting Heart Disease Severity through Machine Learning"
  },
  {
    "objectID": "python/abTest.html",
    "href": "python/abTest.html",
    "title": "Shady Business: Testing the Power of Dark mode",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport os\nos.getcwd()\ndf = pd.read_csv(\n    'c:\\\\Users\\\\HoraceTsai\\\\Documents\\\\data\\\\ab_testing.csv'\n)\ndf.head()\n\n\n\n\n\n\n\n\n\nUser ID\nGroup\nPage Views\nTime Spent\nConversion\nDevice\nLocation\n\n\n\n\n0\n14292\nB\n3\n424\nNo\nMobile\nNorthern Ireland\n\n\n1\n11682\nA\n9\n342\nNo\nMobile\nScotland\n\n\n2\n19825\nA\n2\n396\nNo\nDesktop\nNorthern Ireland\n\n\n3\n16080\nB\n4\n318\nNo\nDesktop\nWales\n\n\n4\n18851\nA\n1\n338\nYes\nDesktop\nScotland"
  },
  {
    "objectID": "python/abTest.html#exploratory-data-analysis",
    "href": "python/abTest.html#exploratory-data-analysis",
    "title": "Shady Business: Testing the Power of Dark mode",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nsummary_stats = df.groupby(\"Group\")[\"Time Spent\"].describe()\nsummary_stats\n\n#plot \nplt.figure(figsize=(10,6))\nsns.boxplot(\n    x = \"Group\",\n    y = \"Time Spent\",\n    data = df,\n    order = [\"A\", \"B\"],\n    palette = \"Accent\"\n)\nplt.title(\"Distribution of Time Spent by Group\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Time Spent (seconds)\")\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#plot \nplt.figure(figsize=(10,6))\nsns.histplot(\n    df,\n    x = \"Time Spent\",\n    hue = \"Group\",\n    kde = True,\n    bins = 30,\n    palette = \"Accent\",\n    alpha = 0.6,\n)\nplt.title(\"Distribution of Time Spent by Group\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Frequency\")\nplt.legend(title=\"Group\", \n    labels=[\"A (White Background)\", \"B (Black Background)\"], \n    loc='upper left',\n    bbox_to_anchor=(1, 1)\n    )\nplt.grid(False)\nplt.show()\n# fairly similar distribution\n\n\nC:\\Users\\HoraceTsai\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngroup_A = df[df[\"Group\"] == \"A\"][\"Time Spent\"]\ngroup_B = df[df[\"Group\"] == \"B\"][\"Time Spent\"]\n\nt_stat, p_val = stats.ttest_ind(\n    group_A, \n    group_B,\n    equal_var = False\n    )\n\nprint(t_stat, p_val)\n# small t- stat meaning group a (white) has a slightly higher mean of time spent vs group B\n# however difference between the two gorups is not large compared to the variability of the data\n# therefore no signif diff between background colors \n# changing website background does not meaningfully affect how long users stay\n\n\n-0.4694926489675219 0.6387380383087231\n\n\n\n\nCode\n# Convert Conversion column to binary (Yes = 1, No = 0)\ndf[\"Conversion\"] = df[\"Conversion\"].map({\"Yes\": 1, \"No\": 0})\n\ndf.head()\n\n\n\n\n\n\n\n\n\nUser ID\nGroup\nPage Views\nTime Spent\nConversion\nDevice\nLocation\n\n\n\n\n0\n14292\nB\n3\n424\n0\nMobile\nNorthern Ireland\n\n\n1\n11682\nA\n9\n342\n0\nMobile\nScotland\n\n\n2\n19825\nA\n2\n396\n0\nDesktop\nNorthern Ireland\n\n\n3\n16080\nB\n4\n318\n0\nDesktop\nWales\n\n\n4\n18851\nA\n1\n338\n1\nDesktop\nScotland\n\n\n\n\n\n\n\n\n\nCode\n# Calculate conversion rates for each group\nconversion_rates = df.groupby(\"Group\")[\"Conversion\"].mean().reset_index()\n\n# bar plot for conversion rates\nplt.figure(figsize =(8,5))\nsns.barplot(\n    x = \"Group\",\n    y = \"Conversion\",\n    data = conversion_rates,\n    palette = \"Accent\"\n)\n\n# Add annotations\nfor index, row in conversion_rates.iterrows():\n    plt.text(index, row[\"Conversion\"] + 0.01, f\"{row['Conversion']*100:.2f}%\", ha='center', fontsize=12)\n\n# Formatting the plot\nplt.title(\"Conversion Rate by Group (A vs. B)\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Conversion Rate\")\nplt.ylim(0, 1)  # Ensuring y-axis goes from 0 to 1 (100% scale)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Perform a chi-square test for conversion rates\ncontingency_table = pd.crosstab(df[\"Group\"], df[\"Conversion\"])\nchi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n\n# Display conversion rates and test results\nchi2_stat, p_value\n\n\n(106.22812337440538, 6.571736018334222e-25)"
  }
]